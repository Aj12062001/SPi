# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only "../input/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using "Save & Run All" 
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session


import os

print(os.listdir("/kaggle/input"))



print(os.listdir("/kaggle/input/spi-dataset"))



import pandas as pd
import numpy as np
import gc
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import IsolationForest
from functools import reduce

BASE_PATH = "/kaggle/input/spi-dataset/"
CHUNK = 300_000



print("Processing logon.csv ...")

logon_list = []

for c in pd.read_csv(BASE_PATH + "logon.csv", chunksize=CHUNK):
    c['date'] = pd.to_datetime(c['date'])
    
    daily = c.groupby(
        [c['user'], c['date'].dt.date]
    ).agg(
        logon_count=('activity', lambda x: (x == 'Logon').sum()),
        logoff_count=('activity', lambda x: (x == 'Logoff').sum()),
        unique_pcs=('pc', 'nunique')
    ).reset_index()
    
    daily.columns = ['user', 'date', 'logon_count', 'logoff_count', 'unique_pcs']
    logon_list.append(daily)

logon_features = pd.concat(logon_list)
del logon_list; gc.collect()

print("Logon features shape:", logon_features.shape)



print("Processing http.csv ...")

http_list = []

for c in pd.read_csv(BASE_PATH + "http.csv", chunksize=CHUNK):
    c['date'] = pd.to_datetime(c['date']).dt.date
    
    daily = c.groupby(['user', 'date']).agg(
        http_requests=('id', 'count'),
        unique_urls=('url', 'nunique')
    ).reset_index()
    
    http_list.append(daily)

http_features = pd.concat(http_list)
del http_list; gc.collect()

print("HTTP features shape:", http_features.shape)



print("Processing file.csv ...")

file_list = []

for c in pd.read_csv(BASE_PATH + "file.csv", chunksize=CHUNK):
    c['date'] = pd.to_datetime(c['date']).dt.date
    
    daily = c.groupby(['user', 'date']).agg(
        file_events=('id', 'count'),
        file_reads=('activity', lambda x: (x == 'Read').sum()),
        file_writes=('activity', lambda x: (x == 'Write').sum()),
        to_usb=('to_removable_media', 'sum'),
        from_usb=('from_removable_media', 'sum')
    ).reset_index()
    
    file_list.append(daily)

file_features = pd.concat(file_list)
del file_list; gc.collect()

print("File features shape:", file_features.shape)



import pandas as pd

sample = pd.read_csv(
    "/kaggle/input/spi-dataset/file.csv",
    nrows=5
)

print(sample.columns.tolist())
sample.head()



print("Processing file.csv ...")

file_list = []

for c in pd.read_csv(BASE_PATH + "file.csv", chunksize=CHUNK):
    c['date'] = pd.to_datetime(c['date']).dt.date

    daily = c.groupby(['user', 'date']).agg(
        file_events=('id', 'count'),
        unique_files=('filename', 'nunique'),
        avg_filename_length=('filename', lambda x: x.astype(str).str.len().mean())
    ).reset_index()

    file_list.append(daily)

file_features = pd.concat(file_list)
del file_list
import gc; gc.collect()

print("File features shape:", file_features.shape)



print("Processing email.csv ...")

email_list = []

for c in pd.read_csv(BASE_PATH + "email.csv", chunksize=CHUNK):
    c['date'] = pd.to_datetime(c['date']).dt.date
    
    daily = c.groupby(['user', 'date']).agg(
        emails_sent=('id', 'count'),
        avg_email_size=('size', 'mean'),
        attachments=('attachments', lambda x: (x > 0).sum()),
        external_mails=('to', lambda x: x.str.contains("@", na=False).sum())
    ).reset_index()
    
    email_list.append(daily)

email_features = pd.concat(email_list)
del email_list; gc.collect()

print("Email features shape:", email_features.shape)



print("Processing device.csv ...")

device_list = []

for c in pd.read_csv(BASE_PATH + "device.csv", chunksize=CHUNK):
    c['date'] = pd.to_datetime(c['date']).dt.date
    
    daily = c.groupby(['user', 'date']).agg(
        usb_connect=('activity', lambda x: (x == 'Connect').sum()),
        usb_disconnect=('activity', lambda x: (x == 'Disconnect').sum())
    ).reset_index()
    
    device_list.append(daily)

device_features = pd.concat(device_list)
del device_list; gc.collect()

print("Device features shape:", device_features.shape)



print("Processing psychometric.csv ...")

psych = pd.read_csv(BASE_PATH + "psychometric.csv")
psych = psych.rename(columns={'user_id': 'user'})



print(psych.shape)
psych.head()



print("Merging all features ...")

from functools import reduce

dfs = [
    logon_features,
    file_features,
    email_features,
    device_features,
    http_features
]

final_df = reduce(
    lambda l, r: pd.merge(l, r, on=['user', 'date'], how='left'),
    dfs
)

final_df = pd.merge(final_df, psych, on='user', how='left')
final_df.fillna(0, inplace=True)

print("Final dataset shape:", final_df.shape)
final_df.head()



import pandas as pd
import numpy as np



print("Training Isolation Forest model ...")

X = final_df.drop(columns=['user', 'date'])

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

model = IsolationForest(
    n_estimators=300,
    contamination=0.05,
    random_state=42,
    n_jobs=-1
)

model.fit(X_scaled)

final_df['anomaly'] = model.predict(X_scaled)

print("Training completed")
print(final_df[['user', 'date', 'anomaly']].head())



from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import IsolationForest
import pandas as pd

# Drop columns not used in features
X = final_df.drop(columns=['date'])  # Keep 'user' if you want to encode it

# One-hot encode categorical columns
X = pd.get_dummies(X, columns=['user'], drop_first=True)  # convert 'user' to numeric

# Scale numeric data
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Train Isolation Forest
model = IsolationForest(
    n_estimators=300,
    contamination=0.05,
    random_state=42,
    n_jobs=-1
)
model.fit(X_scaled)

# Predict anomalies
final_df['anomaly'] = model.predict(X_scaled)

print("Training completed")
print(final_df[['user', 'date', 'anomaly']].head())



from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import IsolationForest
import pandas as pd

# Step 1: Drop non-feature columns
X = final_df.drop(columns=['date'])  # Keep 'user' for encoding

# Step 2: Automatically one-hot encode all categorical columns
categorical_cols = X.select_dtypes(include=['object', 'category']).columns
X_encoded = pd.get_dummies(X, columns=categorical_cols, drop_first=True)

# Step 3: Scale numeric data
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_encoded)

# Step 4: Train Isolation Forest
model = IsolationForest(
    n_estimators=300,
    contamination=0.05,
    random_state=42,
    n_jobs=-1
)
model.fit(X_scaled)

# Step 5: Predict anomalies
final_df['anomaly'] = model.predict(X_scaled)

print("Training completed")
print(final_df[['user', 'date', 'anomaly']].head())



print(final_df['anomaly'].value_counts())



final_df.to_csv("/kaggle/working/spi_features_with_anomalies.csv", index=False)
print("Saved spi_features_with_anomalies.csv")

