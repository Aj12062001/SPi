{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":14589452,"sourceType":"datasetVersion","datasetId":9317200}],"dockerImageVersionId":31259,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\n\nprint(os.listdir(\"/kaggle/input\"))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-23T06:23:54.629584Z","iopub.execute_input":"2026-01-23T06:23:54.629905Z","iopub.status.idle":"2026-01-23T06:23:54.640427Z","shell.execute_reply.started":"2026-01-23T06:23:54.629875Z","shell.execute_reply":"2026-01-23T06:23:54.639279Z"}},"outputs":[{"name":"stdout","text":"['spi-dataset']\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"print(os.listdir(\"/kaggle/input/spi-dataset\"))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-23T06:24:24.771033Z","iopub.execute_input":"2026-01-23T06:24:24.771735Z","iopub.status.idle":"2026-01-23T06:24:24.782460Z","shell.execute_reply.started":"2026-01-23T06:24:24.771700Z","shell.execute_reply":"2026-01-23T06:24:24.781621Z"}},"outputs":[{"name":"stdout","text":"['LDAP', 'logon.csv', 'device.csv', 'psychometric.csv', 'http.csv', 'email.csv', 'file.csv']\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport gc\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import IsolationForest\nfrom functools import reduce\n\nBASE_PATH = \"/kaggle/input/spi-dataset/\"\nCHUNK = 300_000\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-23T08:09:15.789706Z","iopub.execute_input":"2026-01-23T08:09:15.790652Z","iopub.status.idle":"2026-01-23T08:09:15.799506Z","shell.execute_reply.started":"2026-01-23T08:09:15.790613Z","shell.execute_reply":"2026-01-23T08:09:15.797567Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"print(\"Processing logon.csv ...\")\n\nlogon_list = []\n\nfor c in pd.read_csv(BASE_PATH + \"logon.csv\", chunksize=CHUNK):\n    c['date'] = pd.to_datetime(c['date'])\n    \n    daily = c.groupby(\n        [c['user'], c['date'].dt.date]\n    ).agg(\n        logon_count=('activity', lambda x: (x == 'Logon').sum()),\n        logoff_count=('activity', lambda x: (x == 'Logoff').sum()),\n        unique_pcs=('pc', 'nunique')\n    ).reset_index()\n    \n    daily.columns = ['user', 'date', 'logon_count', 'logoff_count', 'unique_pcs']\n    logon_list.append(daily)\n\nlogon_features = pd.concat(logon_list)\ndel logon_list; gc.collect()\n\nprint(\"Logon features shape:\", logon_features.shape)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-23T08:09:18.670625Z","iopub.execute_input":"2026-01-23T08:09:18.671415Z","iopub.status.idle":"2026-01-23T08:10:30.551669Z","shell.execute_reply.started":"2026-01-23T08:09:18.671383Z","shell.execute_reply":"2026-01-23T08:10:30.550324Z"}},"outputs":[{"name":"stdout","text":"Processing logon.csv ...\nLogon features shape: (331649, 5)\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"print(\"Processing http.csv ...\")\n\nhttp_list = []\n\nfor c in pd.read_csv(BASE_PATH + \"http.csv\", chunksize=CHUNK):\n    c['date'] = pd.to_datetime(c['date']).dt.date\n    \n    daily = c.groupby(['user', 'date']).agg(\n        http_requests=('id', 'count'),\n        unique_urls=('url', 'nunique')\n    ).reset_index()\n    \n    http_list.append(daily)\n\nhttp_features = pd.concat(http_list)\ndel http_list; gc.collect()\n\nprint(\"HTTP features shape:\", http_features.shape)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-23T08:10:36.332536Z","iopub.execute_input":"2026-01-23T08:10:36.333674Z","iopub.status.idle":"2026-01-23T08:17:21.420414Z","shell.execute_reply.started":"2026-01-23T08:10:36.333633Z","shell.execute_reply":"2026-01-23T08:17:21.419168Z"}},"outputs":[{"name":"stdout","text":"Processing http.csv ...\nHTTP features shape: (401551, 4)\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"import pandas as pd\n\nsample = pd.read_csv(\n    \"/kaggle/input/spi-dataset/file.csv\",\n    nrows=5\n)\n\nprint(sample.columns.tolist())\nsample.head()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-23T08:00:17.626876Z","iopub.execute_input":"2026-01-23T08:00:17.627242Z","iopub.status.idle":"2026-01-23T08:00:17.662878Z","shell.execute_reply.started":"2026-01-23T08:00:17.627214Z","shell.execute_reply":"2026-01-23T08:00:17.661559Z"}},"outputs":[{"name":"stdout","text":"['id', 'date', 'user', 'pc', 'filename', 'content']\n","output_type":"stream"},{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"                         id                 date     user       pc  \\\n0  {L9G8-J9QE34VM-2834VDPB}  01/02/2010 07:23:14  MOH0273  PC-6699   \n1  {H0W6-L4FG38XG-9897XTEN}  01/02/2010 07:26:19  MOH0273  PC-6699   \n2  {M3Z0-O2KK89OX-5716MBIM}  01/02/2010 08:12:03  HPH0075  PC-2417   \n3  {E1I4-S4QS61TG-3652YHKR}  01/02/2010 08:17:00  HPH0075  PC-2417   \n4  {D4R7-E7JL45UX-0067XALT}  01/02/2010 08:24:57  HSB0196  PC-8001   \n\n       filename                                            content  \n0  EYPC9Y08.doc  D0-CF-11-E0-A1-B1-1A-E1 during difficulty over...  \n1  N3LTSU3O.pdf  25-50-44-46-2D carpenters 25 landed strait dis...  \n2  D3D3WC9W.doc  D0-CF-11-E0-A1-B1-1A-E1 union 24 declined impo...  \n3  QCSW62YS.doc  D0-CF-11-E0-A1-B1-1A-E1 becoming period begin ...  \n4  AU75JV6U.jpg                                              FF-D8  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>date</th>\n      <th>user</th>\n      <th>pc</th>\n      <th>filename</th>\n      <th>content</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>{L9G8-J9QE34VM-2834VDPB}</td>\n      <td>01/02/2010 07:23:14</td>\n      <td>MOH0273</td>\n      <td>PC-6699</td>\n      <td>EYPC9Y08.doc</td>\n      <td>D0-CF-11-E0-A1-B1-1A-E1 during difficulty over...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>{H0W6-L4FG38XG-9897XTEN}</td>\n      <td>01/02/2010 07:26:19</td>\n      <td>MOH0273</td>\n      <td>PC-6699</td>\n      <td>N3LTSU3O.pdf</td>\n      <td>25-50-44-46-2D carpenters 25 landed strait dis...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>{M3Z0-O2KK89OX-5716MBIM}</td>\n      <td>01/02/2010 08:12:03</td>\n      <td>HPH0075</td>\n      <td>PC-2417</td>\n      <td>D3D3WC9W.doc</td>\n      <td>D0-CF-11-E0-A1-B1-1A-E1 union 24 declined impo...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>{E1I4-S4QS61TG-3652YHKR}</td>\n      <td>01/02/2010 08:17:00</td>\n      <td>HPH0075</td>\n      <td>PC-2417</td>\n      <td>QCSW62YS.doc</td>\n      <td>D0-CF-11-E0-A1-B1-1A-E1 becoming period begin ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>{D4R7-E7JL45UX-0067XALT}</td>\n      <td>01/02/2010 08:24:57</td>\n      <td>HSB0196</td>\n      <td>PC-8001</td>\n      <td>AU75JV6U.jpg</td>\n      <td>FF-D8</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":17},{"cell_type":"code","source":"print(\"Processing file.csv ...\")\n\nfile_list = []\n\nfor c in pd.read_csv(BASE_PATH + \"file.csv\", chunksize=CHUNK):\n    c['date'] = pd.to_datetime(c['date']).dt.date\n\n    daily = c.groupby(['user', 'date']).agg(\n        file_events=('id', 'count'),\n        unique_files=('filename', 'nunique'),\n        avg_filename_length=('filename', lambda x: x.astype(str).str.len().mean())\n    ).reset_index()\n\n    file_list.append(daily)\n\nfile_features = pd.concat(file_list)\ndel file_list\nimport gc; gc.collect()\n\nprint(\"File features shape:\", file_features.shape)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-23T08:00:56.019128Z","iopub.execute_input":"2026-01-23T08:00:56.020285Z","iopub.status.idle":"2026-01-23T08:01:16.580320Z","shell.execute_reply.started":"2026-01-23T08:00:56.020244Z","shell.execute_reply":"2026-01-23T08:01:16.579179Z"}},"outputs":[{"name":"stdout","text":"Processing file.csv ...\nFile features shape: (45950, 5)\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"print(\"Processing email.csv ...\")\n\nemail_list = []\n\nfor c in pd.read_csv(BASE_PATH + \"email.csv\", chunksize=CHUNK):\n    c['date'] = pd.to_datetime(c['date']).dt.date\n    \n    daily = c.groupby(['user', 'date']).agg(\n        emails_sent=('id', 'count'),\n        avg_email_size=('size', 'mean'),\n        attachments=('attachments', lambda x: (x > 0).sum()),\n        external_mails=('to', lambda x: x.str.contains(\"@\", na=False).sum())\n    ).reset_index()\n    \n    email_list.append(daily)\n\nemail_features = pd.concat(email_list)\ndel email_list; gc.collect()\n\nprint(\"Email features shape:\", email_features.shape)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-23T08:02:08.087623Z","iopub.execute_input":"2026-01-23T08:02:08.088098Z","iopub.status.idle":"2026-01-23T08:04:42.024964Z","shell.execute_reply.started":"2026-01-23T08:02:08.088068Z","shell.execute_reply":"2026-01-23T08:04:42.023844Z"}},"outputs":[{"name":"stdout","text":"Processing email.csv ...\nEmail features shape: (329654, 6)\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"print(\"Processing device.csv ...\")\n\ndevice_list = []\n\nfor c in pd.read_csv(BASE_PATH + \"device.csv\", chunksize=CHUNK):\n    c['date'] = pd.to_datetime(c['date']).dt.date\n    \n    daily = c.groupby(['user', 'date']).agg(\n        usb_connect=('activity', lambda x: (x == 'Connect').sum()),\n        usb_disconnect=('activity', lambda x: (x == 'Disconnect').sum())\n    ).reset_index()\n    \n    device_list.append(daily)\n\ndevice_features = pd.concat(device_list)\ndel device_list; gc.collect()\n\nprint(\"Device features shape:\", device_features.shape)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-23T08:05:05.841368Z","iopub.execute_input":"2026-01-23T08:05:05.842108Z","iopub.status.idle":"2026-01-23T08:05:19.241326Z","shell.execute_reply.started":"2026-01-23T08:05:05.842076Z","shell.execute_reply":"2026-01-23T08:05:19.240180Z"}},"outputs":[{"name":"stdout","text":"Processing device.csv ...\nDevice features shape: (55754, 4)\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"print(\"Processing psychometric.csv ...\")\n\npsych = pd.read_csv(BASE_PATH + \"psychometric.csv\")\npsych = psych.rename(columns={'user_id': 'user'})\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-23T08:05:46.621215Z","iopub.execute_input":"2026-01-23T08:05:46.621540Z","iopub.status.idle":"2026-01-23T08:05:46.633081Z","shell.execute_reply.started":"2026-01-23T08:05:46.621514Z","shell.execute_reply":"2026-01-23T08:05:46.631570Z"}},"outputs":[{"name":"stdout","text":"Processing psychometric.csv ...\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"print(psych.shape)\npsych.head()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-23T08:07:33.469302Z","iopub.execute_input":"2026-01-23T08:07:33.470756Z","iopub.status.idle":"2026-01-23T08:07:33.484026Z","shell.execute_reply.started":"2026-01-23T08:07:33.470621Z","shell.execute_reply":"2026-01-23T08:07:33.482869Z"}},"outputs":[{"name":"stdout","text":"(1000, 7)\n","output_type":"stream"},{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"             employee_name     user   O   C   E   A   N\n0         Calvin Edan Love  CEL0561  40  39  36  19  40\n1  Christine Reagan Deleon  CRD0624  26  22  17  39  32\n2    Jade Felicia Caldwell  JFC0557  22  16  23  40  33\n3   Aquila Stewart Dejesus  ASD0577  40  48  36  14  37\n4        Micah Abdul Rojas  MAR0955  36  44  23  44  25","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>employee_name</th>\n      <th>user</th>\n      <th>O</th>\n      <th>C</th>\n      <th>E</th>\n      <th>A</th>\n      <th>N</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Calvin Edan Love</td>\n      <td>CEL0561</td>\n      <td>40</td>\n      <td>39</td>\n      <td>36</td>\n      <td>19</td>\n      <td>40</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Christine Reagan Deleon</td>\n      <td>CRD0624</td>\n      <td>26</td>\n      <td>22</td>\n      <td>17</td>\n      <td>39</td>\n      <td>32</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Jade Felicia Caldwell</td>\n      <td>JFC0557</td>\n      <td>22</td>\n      <td>16</td>\n      <td>23</td>\n      <td>40</td>\n      <td>33</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Aquila Stewart Dejesus</td>\n      <td>ASD0577</td>\n      <td>40</td>\n      <td>48</td>\n      <td>36</td>\n      <td>14</td>\n      <td>37</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Micah Abdul Rojas</td>\n      <td>MAR0955</td>\n      <td>36</td>\n      <td>44</td>\n      <td>23</td>\n      <td>44</td>\n      <td>25</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":23},{"cell_type":"code","source":"print(\"Merging all features ...\")\n\nfrom functools import reduce\n\ndfs = [\n    logon_features,\n    file_features,\n    email_features,\n    device_features,\n    http_features\n]\n\nfinal_df = reduce(\n    lambda l, r: pd.merge(l, r, on=['user', 'date'], how='left'),\n    dfs\n)\n\nfinal_df = pd.merge(final_df, psych, on='user', how='left')\nfinal_df.fillna(0, inplace=True)\n\nprint(\"Final dataset shape:\", final_df.shape)\nfinal_df.head()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-23T08:17:51.621392Z","iopub.execute_input":"2026-01-23T08:17:51.622368Z","iopub.status.idle":"2026-01-23T08:17:52.712748Z","shell.execute_reply.started":"2026-01-23T08:17:51.622333Z","shell.execute_reply":"2026-01-23T08:17:52.711641Z"}},"outputs":[{"name":"stdout","text":"Merging all features ...\nFinal dataset shape: (407112, 22)\n","output_type":"stream"},{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"      user        date  logon_count  logoff_count  unique_pcs  file_events  \\\n0  AAE0190  2010-01-04            1             1           1          0.0   \n1  AAE0190  2010-01-05            1             1           1          0.0   \n2  AAE0190  2010-01-06            1             1           1          0.0   \n3  AAE0190  2010-01-07            1             1           1          0.0   \n4  AAE0190  2010-01-07            1             1           1          0.0   \n\n   unique_files  avg_filename_length  emails_sent  avg_email_size  ...  \\\n0           0.0                  0.0         14.0    31523.428571  ...   \n1           0.0                  0.0         13.0    27350.153846  ...   \n2           0.0                  0.0         14.0    38046.214286  ...   \n3           0.0                  0.0         14.0    33902.214286  ...   \n4           0.0                  0.0         14.0    33902.214286  ...   \n\n   usb_connect  usb_disconnect  http_requests  unique_urls  \\\n0          0.0             0.0          143.0         54.0   \n1          0.0             0.0          143.0         53.0   \n2          0.0             0.0          143.0         51.0   \n3          0.0             0.0           43.0         24.0   \n4          0.0             0.0          100.0         39.0   \n\n          employee_name   O   C   E   A   N  \n0  August Armando Evans  36  30  14  50  29  \n1  August Armando Evans  36  30  14  50  29  \n2  August Armando Evans  36  30  14  50  29  \n3  August Armando Evans  36  30  14  50  29  \n4  August Armando Evans  36  30  14  50  29  \n\n[5 rows x 22 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>user</th>\n      <th>date</th>\n      <th>logon_count</th>\n      <th>logoff_count</th>\n      <th>unique_pcs</th>\n      <th>file_events</th>\n      <th>unique_files</th>\n      <th>avg_filename_length</th>\n      <th>emails_sent</th>\n      <th>avg_email_size</th>\n      <th>...</th>\n      <th>usb_connect</th>\n      <th>usb_disconnect</th>\n      <th>http_requests</th>\n      <th>unique_urls</th>\n      <th>employee_name</th>\n      <th>O</th>\n      <th>C</th>\n      <th>E</th>\n      <th>A</th>\n      <th>N</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>AAE0190</td>\n      <td>2010-01-04</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>14.0</td>\n      <td>31523.428571</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>143.0</td>\n      <td>54.0</td>\n      <td>August Armando Evans</td>\n      <td>36</td>\n      <td>30</td>\n      <td>14</td>\n      <td>50</td>\n      <td>29</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>AAE0190</td>\n      <td>2010-01-05</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>13.0</td>\n      <td>27350.153846</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>143.0</td>\n      <td>53.0</td>\n      <td>August Armando Evans</td>\n      <td>36</td>\n      <td>30</td>\n      <td>14</td>\n      <td>50</td>\n      <td>29</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>AAE0190</td>\n      <td>2010-01-06</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>14.0</td>\n      <td>38046.214286</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>143.0</td>\n      <td>51.0</td>\n      <td>August Armando Evans</td>\n      <td>36</td>\n      <td>30</td>\n      <td>14</td>\n      <td>50</td>\n      <td>29</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>AAE0190</td>\n      <td>2010-01-07</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>14.0</td>\n      <td>33902.214286</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>43.0</td>\n      <td>24.0</td>\n      <td>August Armando Evans</td>\n      <td>36</td>\n      <td>30</td>\n      <td>14</td>\n      <td>50</td>\n      <td>29</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>AAE0190</td>\n      <td>2010-01-07</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>14.0</td>\n      <td>33902.214286</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>100.0</td>\n      <td>39.0</td>\n      <td>August Armando Evans</td>\n      <td>36</td>\n      <td>30</td>\n      <td>14</td>\n      <td>50</td>\n      <td>29</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 22 columns</p>\n</div>"},"metadata":{}}],"execution_count":28},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-23T08:18:37.877255Z","iopub.execute_input":"2026-01-23T08:18:37.877805Z","iopub.status.idle":"2026-01-23T08:18:37.883635Z","shell.execute_reply.started":"2026-01-23T08:18:37.877770Z","shell.execute_reply":"2026-01-23T08:18:37.882229Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import IsolationForest\nimport pandas as pd\n\n# Step 1: Drop non-feature columns\nX = final_df.drop(columns=['date'])  # Keep 'user' for encoding\n\n# Step 2: Automatically one-hot encode all categorical columns\ncategorical_cols = X.select_dtypes(include=['object', 'category']).columns\nX_encoded = pd.get_dummies(X, columns=categorical_cols, drop_first=True)\n\n# Step 3: Scale numeric data\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X_encoded)\n\n# Step 4: Train Isolation Forest\nmodel = IsolationForest(\n    n_estimators=300,\n    contamination=0.05,\n    random_state=42,\n    n_jobs=-1\n)\nmodel.fit(X_scaled)\n\n# Step 5: Predict anomalies\nfinal_df['anomaly'] = model.predict(X_scaled)\n\nprint(\"Training completed\")\nprint(final_df[['user', 'date', 'anomaly']].head())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-23T08:22:12.219202Z","iopub.execute_input":"2026-01-23T08:22:12.219763Z","iopub.status.idle":"2026-01-23T08:23:10.425787Z","shell.execute_reply.started":"2026-01-23T08:22:12.219730Z","shell.execute_reply":"2026-01-23T08:23:10.424412Z"}},"outputs":[{"name":"stdout","text":"Training completed\n      user        date  anomaly\n0  AAE0190  2010-01-04        1\n1  AAE0190  2010-01-05        1\n2  AAE0190  2010-01-06        1\n3  AAE0190  2010-01-07        1\n4  AAE0190  2010-01-07        1\n","output_type":"stream"}],"execution_count":32},{"cell_type":"code","source":"print(final_df['anomaly'].value_counts())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-23T08:23:51.273197Z","iopub.execute_input":"2026-01-23T08:23:51.273794Z","iopub.status.idle":"2026-01-23T08:23:51.292043Z","shell.execute_reply.started":"2026-01-23T08:23:51.273757Z","shell.execute_reply":"2026-01-23T08:23:51.289731Z"}},"outputs":[{"name":"stdout","text":"anomaly\n 1    386756\n-1     20356\nName: count, dtype: int64\n","output_type":"stream"}],"execution_count":33},{"cell_type":"code","source":"final_df.to_csv(\"/kaggle/working/spi_features_with_anomalies.csv\", index=False)\nprint(\"Saved spi_features_with_anomalies.csv\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-23T08:25:19.109822Z","iopub.execute_input":"2026-01-23T08:25:19.111645Z","iopub.status.idle":"2026-01-23T08:25:26.969555Z","shell.execute_reply.started":"2026-01-23T08:25:19.111501Z","shell.execute_reply":"2026-01-23T08:25:26.968189Z"}},"outputs":[{"name":"stdout","text":"Saved spi_features_with_anomalies.csv\n","output_type":"stream"}],"execution_count":34},{"cell_type":"code","source":"import joblib\n\n# Save model\njoblib.dump(model, '/kaggle/working/spi_isolation_forest_model.pkl')\nprint(\"Model saved as spi_isolation_forest_model.pkl\")\n\n# Load it later\n# loaded_model = joblib.load('/kaggle/working/spi_isolation_forest_model.pkl')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-23T08:27:42.209587Z","iopub.execute_input":"2026-01-23T08:27:42.210167Z","iopub.status.idle":"2026-01-23T08:27:42.589178Z","shell.execute_reply.started":"2026-01-23T08:27:42.210130Z","shell.execute_reply":"2026-01-23T08:27:42.587680Z"}},"outputs":[{"name":"stdout","text":"Model saved as spi_isolation_forest_model.pkl\n","output_type":"stream"}],"execution_count":35}]}